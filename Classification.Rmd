---
title: "Klassification"
author: "Kate"
date: '11 декабря 2018 г '
output: html_document
---

```{r setup, include=FALSE}
library(randomForest)
library(ISLR)
library(brr)
library(tree)
library(stats)
library(e1071)
```

# Внимание! 
# Внимание! 
# Внимание! 
# Биномиальная классификация ниже! Она есть!

## А пока, строим просто деревья

### Загрузим наши данные: (Беру данные по наркоманам, посмотрю как классифицируется. например, разобъет по месту лечения: амбулаторно и диспансер. Итак)

```{r}
Data <- read.csv("D:/Учеба/1 КУРС МАГИСТРАТУРА/Алексеева/DATA/DataNark.csv", header = TRUE, sep = ";",dec = ",")
head(Data)
Data$intpla <- as.factor(Data$intpla)
Data$tlfbh2 <- as.numeric(Data$tlfbh2)
str(Data)
```

Разобьем на тестовую и обучающую:

```{r}
#сделаем рандомно это, перемешиваем индексы:
n <- nrow(Data)
index <- sample(n)
#т.е. 272/3=90,...   возьмем первые 90 для теста, остальные для обучения
```


```{r}
DataRand <- Data[index,]
DataTest <- DataRand[1:90,]
DataTrain <- DataRand[(90+1):nrow(Data),]
```


```{r}
Data.rf <- randomForest(intpla~ ., data = DataTrain, ntree = 500, proximity=TRUE)
table(predict(Data.rf), DataTrain$intpla) 
```

Посмотрим на ошибку:

```{r}
print(Data.rf)
```


Смотрим на информативность признаков:
```{r}
importance(Data.rf) 
```




Тут вроде и все более менее логично - пол наименее информативен! А социальный статус наиболее важен, что тоже может быть логичным, на картинке ниже все видно более наглядно:

```{r}
varImpPlot(Data.rf)
```


## Проверка обученной модели на тестовых данных.

```{r}
Pred <- predict(Data.rf, newdata = DataTest)
table(Pred, DataTest$intpla)
```




# Построим дерево классификаций:

```{r}
treeMy <- tree(intpla~ ., data = DataTrain)
summary(treeMy)
```

```{r}
plot(treeMy)
text(treeMy)
```

### Вопрос, а почему дерево такое? как оно рисуется? почему вот у se показатель именно 0.5? Это дерево вообщес строится по алгоритму CART (Classification And Regression Tree):

В алгоритме CART каждый узел дерева решений имеет двух потомков. На каждом шаге построения дерева правило, формируемое в узле, делит заданное множество примеров (обучающую выборку) на две части – часть, в которой выполняется правило (потомок – right) и часть, в которой правило не выполняется (потомок – left). Для выбора оптимального правила используется функция оценки качества разбиения.

Обучение дерева решений относится к классу обучения с учителем, то есть обучающая и тестовая выборки содержат классифицированный набор примеров. Оценочная функция, используемая алгоритмом CART, базируется на интуитивной идее уменьшения нечистоты (неопределённости) в узле.

В алгоритме CART идея 'нечистоты' формализована в индексе Gini (который оценивает "расстояние" между распределениями классов.). Если набор данных Т содержит данные n классов, тогда индекс Gini определяется как:

$Gini(T) = 1-\sum_{i=1}^np_i^2$, где $p_i$ --- вероятность(относительная частота) класса i в T.

Если набор T разбивается на 2 части $T_1$ и $T_2$ с числом примеров в каждом $N_1$ и $N_2$ соответственно, тогда показатель качества разбиения будет равен:

$$Gini_{split}(T)=\frac{N_1}{N}Gini(T_1)+\frac{N_2}{N}Gini(T_2)$$

Наилучшим считается то разбиение, для которого $Gini_{split}(T)$ минимально.

Обозначим N – число примеров в узле – предке, L, R – число примеров соответственно в левом и правом потомке, $l_i$ и $r_i$ – число экземпляров i-го класса в левом/правом потомке. Тогда качество разбиения оценивается по следующей формуле:

$$Gini_{split}=\frac{L}{N}(1-\sum_{i=1}^n(\frac{l_i}{L})^2)+\frac{R}{N}(1-\sum_{i=1}^n(\frac{r_i}{L})^2) -> min$$

Чтобы уменьшить объем вычислений формулу можно преобразовать:


$$Gini_{split}=\frac{1}{N}(L(1-\frac{1}{L^2}\sum_{i=1}^nl_i^2)+R(1-\frac{1}{R^2}\sum_{i=1}^nr_i^2)) -> min$$

Так как умножение на константу не играет роли при минимизации:

$$Gini_{split}=L -\frac{1}{L}\sum_{i=1}^nl_i^2+R-\frac{1}{R}\sum_{i=1}^nr_i^2 -> min$$

$$Gini_{split}=N -(\frac{1}{L}\sum_{i=1}^nl_i^2+\frac{1}{R}\sum_{i=1}^nr_i^2) -> min$$

$$\hat{G}_{split}=\frac{1}{L}\sum_{i=1}^nl_i^2+\frac{1}{R}\sum_{i=1}^nr_i^2 -> max$$

В итоге, лучшим будет то разбиение, для которого величина $\hat{G}_{split}$ максимальна.

Если переменная числового типа, то в узле формируется правило вида $x_i <= c$. Где $с$ – некоторый порог, который чаще всего выбирается как среднее арифметическое двух соседних упорядоченных значений переменной $x_i$ обучающей выборки.



# Интересно, а как SVM справится с этими данными?


```{r}
model <- svm(intpla ~ ., data = DataTrain)
print(model)
summary(model)
```


Проверим точность классификации:


```{r}
table(DataTrain$intpla == predict(model, subset(DataTrain), select = -intpla))
```

На самом деле справилось немного лучше!
Ведь впрошлом у нас было вот что:

```{r}
table(predict(Data.rf), DataTrain$intpla) 
```


Проверим svm на тестовых данных:
```{r}
table(DataTest$intpla == predict(model, subset(DataTest), select = -intpla))
```

Как то так, спасибо за внимание :)


# ВНИМАНИЕ

# Классификация в случае биномиальных распределений:

```{r}
Data <- read.csv("D:/Учеба/1 КУРС МАГИСТРАТУРА/Алексеева/DATA/DataNark.csv", header = TRUE, sep = ";",dec = ",")
```

```{r, echo=FALSE}
Data$intpla <- as.factor(Data$intpla)
Data$sex <- as.factor(Data$sex)
Data$curwor <- as.factor(Data$curwor)
Data$st <- as.factor(Data$st)
Data$ha <- as.factor(Data$ha)
Data$se <- as.factor(Data$se)
Data$end <- as.factor(Data$end)
Data <- na.omit(Data)
```

Пусть у нас имеется p дихотомических признаков $X_1,..,X_p$, у нас p = 5, также у нас k=2: $W_1, W_2$:

```{r}
DataBinFull <- Data[,c("intpla", "sex","curwor","st", "ha", "se")]
head(DataBinFull)
```

Разделим выборку на тестовую и обучающую:

```{r}
#сделаем рандомно это, перемешиваем индексы:
n <- nrow(DataBinFull)
index <- sample(n)
#т.е. 272/3=90,...   возьмем первые 90 для теста, остальные для обучения

DataBinRand <- DataBinFull[index,]
DataBinTest <- DataBinRand[1:90,]
DataBinTrain <- DataBinRand[(90+1):nrow(DataBinFull),]
```


Положим $P(X_j=1|W_i)=p_{ij}$ и $P(X_j=0|W_i)=1-p_{ij}$.
Посчитаем эту матрицу $p_{ij}$:

для подсчета вероятности для начала изобрету велосипед (наверное), (не бейте):

```{r}
Data1 <- DataBinTrain[DataBinTrain[,1]=="1",] # чисто табличка для класса 1 (амбулаторно)
Data2 <- DataBinTrain[DataBinTrain[,1]=="2",] # чисто табличка для класса 2 (диспансер)
head(Data1)
head(Data2)
```

### Считаем оценки $p_{ij}=\frac{n_{ij}}{n_i}$

```{r}
p <- matrix(5, nrow = 2, ncol = 5)
  for(j in (1:5)){
    p[1,j] <- nrow(Data1[Data1[,j+1]=="1",])/nrow(Data1)
    p[2,j] <- nrow(Data2[Data2[,j+1]=="1",])/nrow(Data2)
  }

p
```

(хотя вроде нормально получилось)

### Итак, теперь считаем $q_i=n_i/n$: (это оценки априорных вероятностей)

```{r}
q1 <- nrow(Data1)/nrow(DataBinTrain)
q2 <- nrow(Data2)/nrow(DataBinTrain)
q1
q2
```

## Давайте ка напишем функцию, которая будет проводить классификацию:


```{r}
KlassBin <- function(DataFun){ #Напишем для начала просто функцию, которую потом запусим!

f1 <- 1 #потому что умножаем
f2 <- 1 
# f1 и f2 - это вероятности появления определенного вектора X, при условии W1 и W2 соответственно

m <- matrix(0, nrow = 2, ncol = 2) # Завели еще сразу матрицу 2*2 в которую будем заносить потихоньку результаты

for (i in (1:nrow(DataFun))){
  
  # Тут считаем f1(X) и f2(x):
  for (j in (1:5)){
    if (DataFun[i,j+1] == "1"){
      f1 <- f1*p[1,j]
      f2 <- f2*p[2,j]
    }
    if (DataFun[i,j+1] == "0"){
      f1 <- f1*(1-p[1,j])
      f2 <- f2*(1-p[2,j])
    }
  }
  
  # Посчитаем теперь P(X) - вероятность получения вектора X:
  
  P_X <- q1*f1+q2*f2

  # Посчитаем апостериорные вероятности P(W1|X) и P(W2|X):
  
  P_W1_X <- (q1*f1)/P_X
  P_W2_X <- (q2*f2)/P_X
  
  
  
  # Смотрим, куда отнести эту штуку (как классифицировать):
  
  if(P_W1_X>P_W2_X){
    # В этом условии мы классифицируем это как класс W1
    # И сейчас хотим занести в матрицу m результат, правильно или нет классифицировали:
    # т.е. мы сравниваем с правильным ответом:
    if(DataFun[i,1] == "1"){  # Было из класса W1 и классифицировали так же!
      m[1,1] <- m[1,1]+1
    }
    if(DataFun[i,1] == "2"){ # Было из класса W2, но классифицировали как W1!
      m[2,1] <- m[2,1]+1
    }
  }
    else{
    # В этом условии мы классифицируем это как класс W2
    # И сейчас хотим занести в матрицу m результат, правильно или нет классифицировали:
    # т.е. мы сравниваем с правильным ответом:
    if(DataFun[i,1] == "2"){  # Было из класса W2 и классифицировали так же!
      m[2,2] <- m[2,2]+1
    }
    if(DataFun[i,1] == "2"){ # Было из класса W1, но классифицировали как W2!
      m[1,2] <- m[1,2]+1
    }
  }
}

# ну вот а тут просто на вывод функции будет матрица m

 return(m)

}
```



## Ну давайте запустим это на обучающей выборке:

```{r}
KlassBin(DataBinTrain)
```

Ну вот так вот плохо с этим справляется такая классификация....

Давайте еще все же глянем на тестовые данные:

```{r}
KlassBin(DataBinTest)
```

Внимание, Вопроооос, а как же SVM ???

```{r}
model <- svm(intpla ~ ., data = DataBinTrain)
table(DataBinTrain$intpla == predict(model, subset(DataBinTrain), select = -intpla))
```

Ну вот SVM справился горааааздо лучше!





SVM:

```{r}
table(DataBinTest$intpla == predict(model, subset(DataBinTest), select = -intpla))
```

Ну тоже гораздо лучше!










